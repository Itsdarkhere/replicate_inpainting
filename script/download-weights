#!/usr/bin/env python
import torch
import json
import os
from diffusers import StableDiffusionInpaintPipeline
from transformers import CLIPTextModel, CLIPTokenizer
from huggingface_hub import hf_hub_download

with open("concepts.txt") as infile:
    CONCEPTS = [line.rstrip() for line in infile]

model_id = "stabilityai/stable-diffusion-2-inpainting"
auth_token = "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"
os.makedirs("pretrain/diffusers-cache", exist_ok=True)
os.makedirs("pretrain/tokenizer", exist_ok=True)
os.makedirs("pretrain/text_encoder", exist_ok=True)

tokenizer = CLIPTokenizer.from_pretrained(
    model_id,
    subfolder="tokenizer",
    cache_dir="pretrain/tokenizer",
    use_auth_token=auth_token,
)
text_encoder = CLIPTextModel.from_pretrained(
    model_id,
    subfolder="text_encoder",
    cache_dir="pretrain/text_encoder",
    use_auth_token=auth_token,
)
# HF/DIFFUSERS: A classical use case of this functionality is to swap the scheduler. 
# Stable Diffusion v1-5 uses the PNDMScheduler by default which is generally not the most performant scheduler. 
# Since the release of stable diffusion, multiple improved schedulers have been published. 
# To use those, the user has to manually load their preferred scheduler and pass it into DiffusionPipeline.from_pretrained().
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    model_id,
    cache_dir="pretrain/diffusers-cache",
    torch_dtype=torch.float16,
    use_auth_token=auth_token,
)

for concept in CONCEPTS:
    concept = concept.split(":")[0]
    os.makedirs(concept, exist_ok=True)
    embeds_path = hf_hub_download(repo_id=concept, filename="learned_embeds.bin", cache_dir=concept, use_auth_token=auth_token)
    token_path = hf_hub_download(repo_id=concept, filename="token_identifier.txt", cache_dir=concept, use_auth_token=auth_token)
    
    with open(token_path, 'r') as file:
        placeholder = file.read()
    print(f"{concept}: {placeholder}")
